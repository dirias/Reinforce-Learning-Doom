{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6012131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from time import sleep, time\n",
    "\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "import vizdoom as vzd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee798e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Q-learning\n",
    "learning_rate = 0.00025  # Tasa de aprendizaje\n",
    "discount_factor = 0.99  # Factor de descuento que determina la importancia de las recompensas futuras.\n",
    "train_epochs = 5  # Número de épocas de entrenamiento.\n",
    "learning_steps_per_epoch = 2000  # Número de pasos de aprendizaje por época.\n",
    "replay_memory_size = 10000  # Tamaño de la memoria de repetición utilizada en el algoritmo.\n",
    "\n",
    "# Configuración de aprendizaje de la red neuronal\n",
    "batch_size = 64  # Tamaño del lote utilizado en el entrenamiento de la red neuronal.\n",
    "\n",
    "# Régimen de entrenamiento\n",
    "test_episodes_per_epoch = 100  # Número de episodios de prueba por época.\n",
    "\n",
    "# Otros parámetros\n",
    "frame_repeat = 12  # Número de repeticiones de fotogramas.\n",
    "resolution = (30, 45)  # Resolución de la pantalla.\n",
    "episodes_to_watch = 10  # Número de episodios para ver.\n",
    "\n",
    "# Opciones de almacenamiento y carga de modelos\n",
    "# model_savefile = \"./model-doom.pth\"  # Ruta para guardar el modelo (comentado).\n",
    "save_model = True  # Indica si se debe guardar el modelo.\n",
    "load_model = False  # Indica si se debe cargar un modelo previamente guardado.\n",
    "skip_learning = False  # Indica si se debe omitir el proceso de aprendizaje.\n",
    "\n",
    "# Ruta del archivo de configuración \n",
    "# config_file_path = os.path.join(vzd.scenarios_path, \"simpler_basic.cfg\")\n",
    "# config_file_path = os.path.join(vzd.scenarios_path, \"rocket_basic.cfg\")\n",
    "# config_file_path = os.path.join(vzd.scenarios_path, \"basic.cfg\")\n",
    "\n",
    "# Lista de escenarios con sus archivos de configuración y modelos asociados\n",
    "list_scenarios = [\n",
    "     {\n",
    "         'config_file': os.path.join(vzd.scenarios_path, \"simpler_basic.cfg\"),\n",
    "         'model_file': './model-doom.pth'\n",
    "     },\n",
    "    # {\n",
    "    #     'config_file': os.path.join(vzd.scenarios_path, \"rocket_basic.cfg\"),\n",
    "    #     'model_file': './model-doom2.pth'\n",
    "    # },\n",
    "    # {\n",
    "    #     'config_file': os.path.join(vzd.scenarios_path, \"basic.cfg\"),\n",
    "    #     'model_file': './model-doom3.pth'\n",
    "    # },\n",
    "    #{\n",
    "    #    'config_file': os.path.join(vzd.scenarios_path, \"health_gathering.cfg\"),\n",
    "    #    'model_file': './model-doom4.pth'\n",
    "    #},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1526957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica si la GPU está disponible y configura el dispositivo\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\") # Utiliza GPU si está disponible\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\") # Utiliza CPU si no hay GPU disponible\n",
    "\n",
    "\n",
    "def preprocess(img):\n",
    "    \"\"\"Redimensiona la imagen a la resolución especificada\"\"\"\n",
    "    img = skimage.transform.resize(img, resolution)\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_simple_game(config_file='./tmp.pth'):\n",
    "    \"\"\"Inicializa el entorno del juego\"\"\"\n",
    "    print(\"Initializing doom...\")\n",
    "    game = vzd.DoomGame()\n",
    "    game.load_config(config_file)\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "    game.init()\n",
    "    print(\"Doom initialized.\")\n",
    "\n",
    "    return game\n",
    "\n",
    "\n",
    "def test(game, agent):\n",
    "    \"\"\"Ejecuta episodios de prueba y muestra los resultados\"\"\"\n",
    "    print(\"\\nTesting...\")\n",
    "    test_scores = []\n",
    "    for test_episode in trange(test_episodes_per_epoch, leave=False):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            game.make_action(actions[best_action_index], frame_repeat)\n",
    "        r = game.get_total_reward()\n",
    "        test_scores.append(r)\n",
    "\n",
    "    test_scores = np.array(test_scores)\n",
    "    print(\n",
    "        \"Results: mean: {:.1f} +/- {:.1f},\".format(\n",
    "            test_scores.mean(), test_scores.std()\n",
    "        ),\n",
    "        \"min: %.1f\" % test_scores.min(),\n",
    "        \"max: %.1f\" % test_scores.max(),\n",
    "    )\n",
    "\n",
    "\n",
    "def run(game, agent, actions, num_epochs, frame_repeat, model_file=None, steps_per_epoch=2000):\n",
    "    \"\"\"\n",
    "    Ejecuta el entrenamiento del agente durante varias épocas\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        game.new_episode()\n",
    "        train_scores = []\n",
    "        global_step = 0\n",
    "        print(\"\\nEpoch #\" + str(epoch + 1))\n",
    "\n",
    "        for _ in trange(steps_per_epoch, leave=False):\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            action = agent.get_action(state)\n",
    "            reward = game.make_action(actions[action], frame_repeat)\n",
    "            done = game.is_episode_finished()\n",
    "\n",
    "            if not done:\n",
    "                next_state = preprocess(game.get_state().screen_buffer)\n",
    "            else:\n",
    "                next_state = np.zeros((1, 30, 45)).astype(np.float32)\n",
    "\n",
    "            agent.append_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            if global_step > agent.batch_size:\n",
    "                agent.train()\n",
    "\n",
    "            if done:\n",
    "                train_scores.append(game.get_total_reward())\n",
    "                game.new_episode()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        agent.update_target_net()\n",
    "        train_scores = np.array(train_scores)\n",
    "\n",
    "        print(\n",
    "            \"Results: mean: {:.1f} +/- {:.1f},\".format(\n",
    "                train_scores.mean(), train_scores.std()\n",
    "            ),\n",
    "            \"min: %.1f,\" % train_scores.min(),\n",
    "            \"max: %.1f,\" % train_scores.max(),\n",
    "        )\n",
    "\n",
    "        test(game, agent) \n",
    "        # Entrena el juego con el agente y guarda los pesos\n",
    "        if save_model:\n",
    "            print(\"Saving the network weights to:\", model_file)\n",
    "            torch.save(agent.q_net, model_file)\n",
    "        print(\"Total elapsed time: %.2f minutes\" % ((time() - start_time) / 60.0))\n",
    "\n",
    "    game.close()\n",
    "    return agent, game\n",
    "\n",
    "\n",
    "class DuelQNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Define una red neuronal con arquitectura Duel DQN\n",
    "    see https://arxiv.org/abs/1511.06581 for more information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, available_actions_count):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.state_fc = nn.Sequential(nn.Linear(96, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "\n",
    "        self.advantage_fc = nn.Sequential(\n",
    "            nn.Linear(96, 64), nn.ReLU(), nn.Linear(64, available_actions_count)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(-1, 192)\n",
    "        x1 = x[:, :96]  # input for the net to calculate the state value\n",
    "        x2 = x[:, 96:]  # relative advantage of actions in the state\n",
    "        state_value = self.state_fc(x1).reshape(-1, 1)\n",
    "        advantage_values = self.advantage_fc(x2)\n",
    "        x = state_value + (\n",
    "            advantage_values - advantage_values.mean(dim=1).reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"Inicializa el agente con hiperparámetros\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_size,\n",
    "        memory_size,\n",
    "        batch_size,\n",
    "        discount_factor,\n",
    "        lr,\n",
    "        load_model,\n",
    "        model_file=None,\n",
    "        epsilon=1,\n",
    "        epsilon_decay=0.9996,\n",
    "        epsilon_min=0.1,\n",
    "    ):\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount_factor\n",
    "        self.lr = lr\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        if load_model:\n",
    "            print(\"Loading model from: \", model_file)\n",
    "            self.q_net = torch.load(model_file)\n",
    "            self.target_net = torch.load(model_file)\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        else:\n",
    "            print(\"Initializing new model\")\n",
    "            self.q_net = DuelQNet(action_size).to(DEVICE)\n",
    "            self.target_net = DuelQNet(action_size).to(DEVICE)\n",
    "\n",
    "        self.opt = optim.SGD(self.q_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            state = torch.from_numpy(state).float().to(DEVICE)\n",
    "            action = torch.argmax(self.q_net(state)).item()\n",
    "            return action\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        batch = np.array(batch, dtype=object)\n",
    "\n",
    "        states = np.stack(batch[:, 0]).astype(float)\n",
    "        actions = batch[:, 1].astype(int)\n",
    "        rewards = batch[:, 2].astype(float)\n",
    "        next_states = np.stack(batch[:, 3]).astype(float)\n",
    "        dones = batch[:, 4].astype(bool)\n",
    "        not_dones = ~dones\n",
    "\n",
    "        row_idx = np.arange(self.batch_size)  # used for indexing the batch\n",
    "\n",
    "        # value of the next states with double q learning\n",
    "        # see https://arxiv.org/abs/1509.06461 for more information on double q learning\n",
    "        with torch.no_grad():\n",
    "            next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "            idx = row_idx, np.argmax(self.q_net(next_states).cpu().data.numpy(), 1)\n",
    "            next_state_values = self.target_net(next_states).cpu().data.numpy()[idx]\n",
    "            next_state_values = next_state_values[not_dones]\n",
    "\n",
    "        # this defines y = r + discount * max_a q(s', a)\n",
    "        q_targets = rewards.copy()\n",
    "        q_targets[not_dones] += self.discount * next_state_values\n",
    "        q_targets = torch.from_numpy(q_targets).float().to(DEVICE)\n",
    "\n",
    "        # this selects only the q values of the actions taken\n",
    "        idx = row_idx, actions\n",
    "        states = torch.from_numpy(states).float().to(DEVICE)\n",
    "        action_values = self.q_net(states)[idx].float().to(DEVICE)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        td_error = self.criterion(q_targets, action_values)\n",
    "        td_error.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2787934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test for scenario /opt/homebrew/Caskroom/miniconda/base/envs/dataScience/lib/python3.10/site-packages/vizdoom/scenarios/simpler_basic.cfg\n",
      "Initializing doom...\n",
      "Doom initialized.\n",
      "Initializing new model\n",
      "\n",
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Run the training for the set number of epochs\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_learning:\n\u001b[0;32m---> 21\u001b[0m     agent, game \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_repeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_repeat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms time to watch!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(game, agent, actions, num_epochs, frame_repeat, model_file, steps_per_epoch)\u001b[0m\n\u001b[1;32m     70\u001b[0m state \u001b[38;5;241m=\u001b[39m preprocess(game\u001b[38;5;241m.\u001b[39mget_state()\u001b[38;5;241m.\u001b[39mscreen_buffer)\n\u001b[1;32m     71\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[0;32m---> 72\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_repeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m done \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mis_episode_finished()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "\n",
    "for scenario in list_scenarios:\n",
    "    print(f'Train and test for scenario {scenario[\"config_file\"]}')\n",
    "    # Initialize game and actions\n",
    "    game = create_simple_game(config_file=scenario['config_file'])\n",
    "    n = game.get_available_buttons_size()\n",
    "    actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "    # Initialize our agent with the set parameters\n",
    "    agent = DQNAgent(\n",
    "        len(actions),\n",
    "        lr=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        memory_size=replay_memory_size,\n",
    "        discount_factor=discount_factor,\n",
    "        load_model=load_model,\n",
    "        model_file=scenario['model_file'],\n",
    "    )\n",
    "\n",
    "    # Run the training for the set number of epochs\n",
    "    if not skip_learning:\n",
    "        agent, game = run(\n",
    "            game,\n",
    "            agent,\n",
    "            actions,\n",
    "            model_file=scenario['model_file'],\n",
    "            num_epochs=train_epochs,\n",
    "            frame_repeat=frame_repeat,\n",
    "            steps_per_epoch=learning_steps_per_epoch,\n",
    "        )\n",
    "\n",
    "        print(\"======================================\")\n",
    "        print(\"Training finished. It's time to watch!\")\n",
    "\n",
    "    # Reinitialize the game with window visible\n",
    "    game.close()\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(vzd.Mode.ASYNC_PLAYER)\n",
    "    game.init()\n",
    "\n",
    "    for _ in range(episodes_to_watch):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            # Instead of make_action(a, frame_repeat) in order to make the animation smooth\n",
    "            game.set_action(actions[best_action_index])\n",
    "            for _ in range(frame_repeat):\n",
    "                game.advance_action()\n",
    "\n",
    "        # Sleep between episodes\n",
    "        sleep(1.0)\n",
    "        score = game.get_total_reward()\n",
    "        print(\"Total score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffd638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
